{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOtTFw3wi9JCx5tmzgDbH7m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/conuko/AI-Guild/blob/main/Session_3_Optimizing_Toast.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explanation of the different solutions\n",
        "\n",
        "###EASY:\n",
        "\n",
        "This implementation loops through all possible values of toast_duration\n",
        "and wait_duration from 1 to 100, calculates the utility for each combination,\n",
        "and keeps track of the parameters with the highest utility. It returns the\n",
        "best parameters once all combinations have been tested.\n",
        "\n",
        "Note that this approach may take some time to run since it needs to test\n",
        "10,000 combinations. However, given the simplicity of the problem and the fact\n",
        "that the function evaluations are fast, this approach should be sufficient.\n",
        "\n",
        "###MEDIUM (Hill climbing algorithm):\n",
        "This implementation starts with a random point in the search space, and repeatedly generates neighboring points and moves to the one with the highest utility, as long as it is better than the current point. The algorithm terminates when it reaches a local maximum, i.e., when all neighboring points have lower utility than the current point.\n",
        "\n",
        "Note that this implementation only searches for local optima, and may not\n",
        "find the global optimum if it is far from the starting point. To mitigate this, one could run the algorithm multiple times with different starting points and take the best result.\n",
        "\n",
        "###HARD (Gradient ascent algorithm):\n",
        "The `find_maximum` function is an implementation of the gradient ascent algorithm to find the optimal values of the `toast_duration`, `wait_duration`, and `power` parameters that maximize the `utility` function.\n",
        "\n",
        "The function takes three optional parameters: `learning_rate`, `max_iterations`, and `tolerance`. `learning_rate` controls the step size of the algorithm, `max_iterations` limits the maximum number of iterations, and tolerance sets the convergence threshold.\n",
        "\n",
        "The algorithm initializes the parameters toast_duration, wait_duration, and power to some initial values. It then calculates the initial `utility` value using the utility function.\n",
        "\n",
        "Next, it enters a loop that repeats until one of two stopping conditions is met. The first condition is that the change in utility value is smaller than tolerance, which indicates that the algorithm has converged to a local maximum. The second condition is that the number of iterations has exceeded `max_iterations`.\n",
        "\n",
        "In each iteration of the loop, the algorithm calculates the gradient of the utility function with respect to each parameter using the partial derivative of the utility function with respect to that parameter. It then updates each parameter by adding the gradient times the learning rate. This update moves the parameters in the direction of the gradient and increases the utility value.\n",
        "\n",
        "After updating the parameters, the algorithm calculates the new utility value and the change in utility value. If the change in utility value is less than the tolerance, the algorithm stops. Otherwise, it updates the current utility value and continues to the next iteration.\n",
        "\n",
        "Finally, the algorithm returns the values of toast_duration, wait_duration, and power that maximize the utility function. These values are the ones that were reached in the last iteration of the loop.\n",
        "\n",
        "It is worth noting that the gradient ascent algorithm is a local search algorithm, and it may converge to a local maximum rather than a global maximum. In this case, the initial values of the parameters could be randomly initialized and the algorithm run multiple times to increase the chances of finding a global maximum.\n",
        "\n"
      ],
      "metadata": {
        "id": "CXUePU2T36W-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "FhkkI0Ff9y1Q"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# Problem Setup: You want the perfect bread\n",
        "# What influences this?\n",
        "#  - how long do you toast it?\n",
        "#  - how long after toasting do you eat the bread?\n",
        "#  - Do you have power? And how much? \n",
        "#  - Which toaster do you use?\n",
        "################################################################################\n",
        "\n",
        "import math\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# the function you are supposed to optimize.\n",
        "# It has the following input:\n",
        "#  toast_duration: duration of toasting in seconds. It is supposed to be an integer between 1 and 100\n",
        "#  wait_duration: duration of waiting after toasting in seconds. It's supposed to be an integer between 1 and 100\n",
        "#  toaster: the number of the toaster you want to use. It's supposed to be an integer, between 1 and 10.\n",
        "#  power: how much power the toaster has (it's supposed to be a floating point number between 0 and 2)\n",
        "################################################################################\n",
        "def utility(toast_duration, wait_duration, power = 1.0,toaster = 1):\n",
        "    # handle input errors\n",
        "    if (not type(toast_duration) is int) and not (1 <= toast_duration <= 100):\n",
        "        raise ValueError(\"toast_duration is not an integer\")\n",
        "    if (not type(wait_duration) is int) and not (1 <= wait_duration <= 100):\n",
        "        raise ValueError(\"wait_duration is not an integer\")\n",
        "    if (not type(toaster) is int) and not (1 <= toaster <= 10):\n",
        "        raise ValueError(\"toaster is not an integer or is not in a valid range\")\n",
        "    if (not type(power) is float) and not (0.0 <= power <= 2.0):\n",
        "        raise ValueError(\"power is not a float or not in the valid range\")\n",
        "\n",
        "    # get toaster specific configuration\n",
        "    hpt = [10,8,15,7,9,2,9,19,92,32][toaster-1]\n",
        "    hpw = [1,4,19,3,20,3,1,4,1,62][toaster-1]\n",
        "    toaster_utility = [1,0.9,0.7,1.3,0.3,0.8,0.5,0.8,3,0.2][toaster-1]\n",
        "\n",
        "    # calculate values\n",
        "    toast_utility = -0.1*(toast_duration-hpt)**2+1\n",
        "    wait_utility = -0.01*(wait_duration-hpw)**2+1\n",
        "    overall_utility = (toast_utility + wait_utility) * toaster_utility\n",
        "\n",
        "    # apply modifier based on electricity\n",
        "    power_factor = math.sin(10*power+math.pi/2 -10) + power*0.2\n",
        "    overall_utility *= power_factor\n",
        "\n",
        "    return overall_utility"
      ],
      "metadata": {
        "id": "tuafvZO7-KHO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# Writing this function is your homework. \n",
        "# The function should return the tuple of parameters that optmizes the function.\n",
        "# \n",
        "# You can implement it in multiple difficulty levels:\n",
        "# easy: \n",
        "#     - implement it with only two parameters: toast_duration and wait_duration\n",
        "#     - e.g., utility(2,3)\n",
        "#     - Implement the function by testing all possible values for these variables.\n",
        "#     - (This state space has only 10000 values, so it shouldn't take too long)\n",
        "#\n",
        "# medium: \n",
        "#    - same as easy, but implement hill climbing\n",
        "#    - see pseudo code\n",
        "# hard: \n",
        "#    - also use the parameter power\n",
        "#    - e.g., utility(2,3,1.2)\n",
        "#    - this introduces the following complications:\n",
        "#        - multiple maxima\n",
        "#        - a continuous parameter\n",
        "#    - implement gradient ascent\n",
        "# very hard:\n",
        "#    - Same as hard, but use repeated search to find all maxima. \n",
        "#    - repeated search: \n",
        "#        - apply gradient descent from different starting points.\n",
        "#    - I think there are 5 maxima. But I'm not sure :-P\n",
        "# prepare to cry:\n",
        "#    - find the optimum for all four parameters\n",
        "#    - define your own algorithm!\n",
        "\n",
        "################################################################################\n",
        "# EASY:\n",
        "def find_maximum():\n",
        "    # initialize maximum utility and corresponding parameters\n",
        "    max_utility = -float(\"inf\")\n",
        "    max_params = None\n",
        "    \n",
        "    # loop over all possible parameter values\n",
        "    for toast_duration in range(1, 101):\n",
        "        for wait_duration in range(1, 101):\n",
        "            # calculate utility for these parameters\n",
        "            params = (toast_duration, wait_duration)\n",
        "            curr_utility = utility(*params)\n",
        "            \n",
        "            # update maximum if necessary\n",
        "            if curr_utility > max_utility:\n",
        "                max_utility = curr_utility\n",
        "                max_params = params\n",
        "                \n",
        "    return max_params\n",
        "\n",
        "################################################################################\n",
        "# MEDIUM (HILL CLIMBING):\n",
        "def find_maximum_medium():\n",
        "    # Start with a random point in the search space\n",
        "    curr_params = (random.randint(1, 100), random.randint(1, 100))\n",
        "    curr_utility = utility(*curr_params)\n",
        "\n",
        "    # Define a function to generate neighboring points\n",
        "    def get_neighbors(params):\n",
        "        neighbors = []\n",
        "        for delta_t in [-1, 1]:\n",
        "            for delta_w in [-1, 1]:\n",
        "                t = params[0] + delta_t\n",
        "                w = params[1] + delta_w\n",
        "                if 1 <= t <= 100 and 1 <= w <= 100:\n",
        "                    neighbors.append((t, w))\n",
        "        return neighbors\n",
        "\n",
        "    # Define a loop to perform hill climbing\n",
        "    while True:\n",
        "        # Generate neighbors and find the one with the highest utility\n",
        "        neighbors = get_neighbors(curr_params)\n",
        "        best_neighbor = max(neighbors, key=lambda x: utility(*x))\n",
        "\n",
        "        # If the best neighbor has higher utility, move to that point\n",
        "        if utility(*best_neighbor) > curr_utility:\n",
        "            curr_params = best_neighbor\n",
        "            curr_utility = utility(*curr_params)\n",
        "        # Otherwise, return the current point as the optimum\n",
        "        else:\n",
        "            return curr_params\n",
        "\n",
        "################################################################################\n",
        "# HARD (GRADIENT ASCENT):\n",
        "def find_maximum_hard(learning_rate=0.01, max_iterations=1000, tolerance=1e-6):\n",
        "    # set initial values for the parameters\n",
        "    toast_duration = 50\n",
        "    wait_duration = 50\n",
        "    power = 1.0\n",
        "\n",
        "    # set the initial utility and the delta utility\n",
        "    current_utility = utility(toast_duration, wait_duration, power)\n",
        "    delta_utility = tolerance + 1\n",
        "\n",
        "    # iterate until convergence or max_iterations\n",
        "    iteration = 0\n",
        "    while delta_utility > tolerance and iteration < max_iterations:\n",
        "        # calculate the gradient for each parameter\n",
        "        d_toast_duration = 0.2*(toast_duration-10)\n",
        "        d_wait_duration = 0.02*(wait_duration-19)\n",
        "        d_power = math.cos(10*power+math.pi/2 -10) + 0.2\n",
        "\n",
        "        # update the parameters using the gradient\n",
        "        toast_duration += learning_rate * d_toast_duration\n",
        "        wait_duration += learning_rate * d_wait_duration\n",
        "        power += learning_rate * d_power\n",
        "\n",
        "        # calculate the new utility and delta utility\n",
        "        new_utility = utility(toast_duration, wait_duration, power)\n",
        "        delta_utility = new_utility - current_utility\n",
        "        current_utility = new_utility\n",
        "\n",
        "        iteration += 1\n",
        "\n",
        "    return (toast_duration, wait_duration, power)\n",
        "\n",
        "################################################################################\n",
        "# HARD SECOND APPROACH (GRADIENT ASCENT):\n",
        "# To handle multiple maxima, we can add random initialization to the parameters\n",
        "# in the find_maximum function. This way, we can run the gradient ascent\n",
        "# algorithm multiple times from different starting points and choose the\n",
        "# parameter values that yield the highest utility value. Additionally,\n",
        "# to handle a continuous parameter, we can modify the function to take\n",
        "# a range of values for that parameter, and iterate over that range to find the\n",
        "# optimal value. Here's an improved version of the find_maximum_hard function:\n",
        "\n",
        "def find_maximum_hard2(learning_rate=0.01, max_iterations=1000, tolerance=1e-6):\n",
        "    # set the range of values for the continuous parameter\n",
        "    power_range = [0.1 * i for i in range(21)]\n",
        "\n",
        "    # set initial values for the parameters\n",
        "    toast_duration = 50\n",
        "    wait_duration = 50\n",
        "    power = 1.0\n",
        "\n",
        "    # set the initial utility and the delta utility\n",
        "    current_utility = utility(toast_duration, wait_duration, power)\n",
        "    delta_utility = tolerance + 1\n",
        "\n",
        "    # iterate until convergence or max_iterations\n",
        "    iteration = 0\n",
        "    while delta_utility > tolerance and iteration < max_iterations:\n",
        "        # calculate the gradient for each parameter\n",
        "        d_toast_duration = 0.2*(toast_duration-10)\n",
        "        d_wait_duration = 0.02*(wait_duration-19)\n",
        "        d_power = math.cos(10*power+math.pi/2 -10) + 0.2\n",
        "\n",
        "        # update the parameters using the gradient\n",
        "        toast_duration += learning_rate * d_toast_duration\n",
        "        wait_duration += learning_rate * d_wait_duration\n",
        "        power += learning_rate * d_power\n",
        "\n",
        "        # make sure the parameter values are within the valid range\n",
        "        toast_duration = max(min(toast_duration, 100), 1)\n",
        "        wait_duration = max(min(wait_duration, 100), 1)\n",
        "        power = max(min(power, 2.0), 0.0)\n",
        "\n",
        "        # calculate the new utility and delta utility\n",
        "        new_utility = utility(toast_duration, wait_duration, power)\n",
        "        delta_utility = new_utility - current_utility\n",
        "        current_utility = new_utility\n",
        "\n",
        "        iteration += 1\n",
        "\n",
        "    # initialize the maximum utility and parameter values\n",
        "    max_utility = current_utility\n",
        "    max_toast_duration = toast_duration\n",
        "    max_wait_duration = wait_duration\n",
        "    max_power = power\n",
        "\n",
        "    # iterate over multiple random initializations to handle multiple maxima\n",
        "    for i in range(10):\n",
        "        # choose a random power value\n",
        "        power = random.choice(power_range)\n",
        "\n",
        "        # set initial values for the other parameters\n",
        "        toast_duration = random.randint(1, 100)\n",
        "        wait_duration = random.randint(1, 100)\n",
        "\n",
        "        # set the initial utility and the delta utility\n",
        "        current_utility = utility(toast_duration, wait_duration, power)\n",
        "        delta_utility = tolerance + 1\n",
        "\n",
        "        # iterate until convergence or max_iterations\n",
        "        iteration = 0\n",
        "        while delta_utility > tolerance and iteration < max_iterations:\n",
        "            # calculate the gradient for each parameter\n",
        "            d_toast_duration = 0.2*(toast_duration-10)\n",
        "            d_wait_duration = 0.02*(wait_duration-19)\n",
        "            d_power = math.cos(10*power+math.pi/2 -10) + 0.2\n",
        "\n",
        "            # update the parameters using the gradient\n",
        "            toast_duration += learning_rate * d_toast_duration\n",
        "            wait_duration += learning_rate * d_wait_duration\n",
        "            power += learning_rate * d_power\n",
        "\n",
        "            # make sure the parameter values are within the valid range\n",
        "            toast_duration = max(min(toast_duration, 100), 1)\n",
        "            wait_duration = max(min(wait_duration, 100), 1)\n",
        "            power = max(min(power, 2.0), 0.0)\n",
        "\n",
        "            # calculate the new utility and delta utility\n",
        "            new_utility = utility(toast_duration, wait_duration, power)\n",
        "            delta_utility = new_utility - current_utility\n",
        "            current_utility = new_utility\n",
        "\n",
        "            iteration += 1\n",
        "\n",
        "            # update the maximum utility and parameter values if necessary\n",
        "            if current_utility > max_utility:\n",
        "                max_utility = current_utility\n",
        "                max_toast_duration = toast_duration\n",
        "                max_wait_duration = wait_duration\n",
        "                max_power = power\n",
        "\n",
        "        return (max_toast_duration, max_wait_duration, max_power)\n",
        "\n"
      ],
      "metadata": {
        "id": "Qrafi6cP-O2X"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use the function and see what it thinks the optimum is\n",
        "optimum = find_maximum()\n",
        "print(\"Optimum:\",optimum,)\n",
        "print(\"value:\",utility(*optimum))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2cxw6Jqgf0R",
        "outputId": "191d51ca-b153-439d-ea7b-65916276736c"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimum: (10, 1)\n",
            "value: 2.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# use the function and see what it thinks the optimum is\n",
        "optimum = find_maximum_medium()\n",
        "print(\"Optimum:\",optimum,)\n",
        "print(\"value:\",utility(*optimum))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6FhnFrTiLGU",
        "outputId": "db735f13-cb9f-4c98-8a4d-fa7f8087632b"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimum: (10, 6)\n",
            "value: 2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# use the function and see what it thinks the optimum is\n",
        "optimum = find_maximum_hard()\n",
        "print(\"Optimum:\",optimum,)\n",
        "print(\"value:\",utility(*optimum))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uM3lyddNizNO",
        "outputId": "3978c9f0-8a4b-4280-d25e-112aeb431be7"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimum: (50.08, 50.0062, 1.002)\n",
            "value: -219.22459222225206\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# use the function and see what it thinks the optimum is\n",
        "optimum = find_maximum_hard2()\n",
        "print(\"Optimum:\",optimum,)\n",
        "print(\"value:\",utility(*optimum))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsgMoH6PjBTS",
        "outputId": "8a26015b-34ab-4743-8f40-549d2ad0e945"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimum: (30.04, 5.9974, 0.10612118485241757)\n",
            "value: 33.14731439244762\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P-Dn0A-075mu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}